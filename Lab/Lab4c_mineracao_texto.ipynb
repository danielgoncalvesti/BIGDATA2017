{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CMCC](http://cmcc.ufabc.edu.br/images/logo_site.jpg)\n",
    "\n",
    "# **Lab 4c - Mineração de Textos**\n",
    "\n",
    "#### Um outro tipo de base de dados que necessita de tratamento especial para representá-la de forma a ser utilizada em algoritmos de aprendizado são as bases textuais.\n",
    "\n",
    "#### Neste notebook aprenderemos a gerar representações utilizadas na literatura.\n",
    "\n",
    "#### As células-exercícios iniciam com o comentário `# EXERCICIO` e os códigos a serem completados estão marcados pelos comentários `<COMPLETAR>`.\n",
    "\n",
    "#### ** Nesse notebook: **\n",
    "#### *Parte 1:* Bag-of-Words\n",
    "#### *Parte 2:* Bag-of-Words ponderados: TF-IDF\n",
    "#### *Parte 3:* Similaridade do Cosseno\n",
    "#### *Parte 4:* k-NN para documentos textuais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 0: Preliminares**\n",
    "\n",
    "#### Para este notebook utilizaremos a base de dados Movie Reviews que será utilizada para o segundo projeto.\n",
    "\n",
    "#### A base de dados tem os campos separados por '\\t' e o seguinte formato:\n",
    "   `\"id da frase\",\"id da sentença\",\"Frase\",\"Sentimento\"`\n",
    "\n",
    "#### Para esse laboratório utilizaremos apenas o campo \"Frase\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def parseRDD(point):\n",
    "    \"\"\" Parser for the current dataset. It receives a data point and return\n",
    "        a sentence (third field).\n",
    "    Args:\n",
    "        point (str): input data point\n",
    "    Returns:\n",
    "        str: a string\n",
    "    \"\"\"    \n",
    "    data = point.split('\\t')\n",
    "    return (int(data[0]),data[2])\n",
    "\n",
    "def notempty(point):\n",
    "    \"\"\" Returns whether the point string is not empty\n",
    "    Args:\n",
    "        point (str): input string\n",
    "    Returns:\n",
    "        bool: True if it is not empty\n",
    "    \"\"\"   \n",
    "    return len(point[1])>0\n",
    "\n",
    "filename = os.path.join(\"Data\",\"Aula04\",\"MovieReviews2.tsv\")\n",
    "rawRDD = sc.textFile(filename,2)\n",
    "header = rawRDD.take(1)[0]\n",
    "\n",
    "dataRDD = (rawRDD\n",
    "           .filter(lambda x: x!=header)\n",
    "           .map(parseRDD)\n",
    "           .filter(notempty)\n",
    "           #.sample( False, 0.1, 42 )\n",
    "           )\n",
    "\n",
    "print 'Read {} lines'.format(dataRDD.count())\n",
    "print 'Sample line: {}'.format(dataRDD.takeSample(False, 1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 1: Bags of Words**\n",
    "\n",
    "#### Uma abordagem simples é transformar as sentenças em vetores esparsos de maneira similar ao One-Hot Encoding. Para isso utilizaremos o conceito de [Bag-of-words][bag-of-words].\n",
    "\n",
    "####  A ideia geral é tratar cada **documento** como uma coleção desordenada de **tokens**.\n",
    "\n",
    "#### Um token é um elemento extraído do texto. Inicialmente podemos pensar neles como uma única palavra, porém temos outros tipos de tokens possíveis conforme veremos mais para frente.\n",
    "\n",
    "[bag-of-words]: https://en.wikipedia.org/wiki/Bag-of-words_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1(a) Tokenizando a String**\n",
    "\n",
    "#### Implemente a função `simpleTokenize(string)` que recebe uma string e retorna uma lista de tokens (palavras) não-vazias dessa string.  `simpleTokenize` deve utilizar expressões regulares para podermos generalizar a limpeza das strings nos próximos passos. Utilizando a função [re.split()](https://docs.python.org/2/library/re.html#re.split) retorne a lista de tokens não-vazios, convertidos em caixa baixa utilizando expressão regular a ser inserida na variável `split_regex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "import re\n",
    "\n",
    "quickbrownfox = 'A quick brown fox jumps over the lazy dog.'\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "def simpleTokenize(string):\n",
    "    \"\"\" A simple implementation of input string tokenization\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens\n",
    "    \"\"\"\n",
    "    return <COMPLETAR>\n",
    "\n",
    "print simpleTokenize(quickbrownfox) # Should give ['a', 'quick', 'brown', ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Tokenize a String (1a)\n",
    "assert simpleTokenize(quickbrownfox)==['a','quick','brown','fox','jumps','over','the','lazy','dog'], 'lista incorreta!'\n",
    "print 'ok!'\n",
    "assert simpleTokenize(' ') == [], 'lista incorreta!'\n",
    "print 'ok!'\n",
    "assert simpleTokenize('!!!!123A/456_B/789C.123A')== ['123a','456_b','789c','123a'],'lista incorreta!'\n",
    "print 'ok!'\n",
    "assert simpleTokenize('fox fox') == ['fox', 'fox'],'lista incorreta'\n",
    "print 'ok!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1b) Removendo stopwords**\n",
    "#### *[Stopwords][stopwords]* são palavras comuns que não contribuem para o significado do texto (ex.: \"e\", \"um\", \"uma\", \"é\", etc.). Permitir que essas palavras permaneçam no texto introduz ruídos durante a comparação de bases textuais.\n",
    "\n",
    "#### Usando o arquivo \"stopwords.txt\", implemente  `tokenize`, uma versão melhorada da função anterior que remove as stopswords.\n",
    "[stopwords]: https://en.wikipedia.org/wiki/Stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "stopfile = os.path.join(\"Data\",\"Aula04\",\"stopwords.txt\")\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "print 'These are the stopwords: %s' % stopwords\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    return <COMPLETAR>\n",
    "\n",
    "print tokenize(quickbrownfox) # Should give ['quick', 'brown', ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Removing stopwords (1b)\n",
    "assert tokenize(\"Why a the?\") == [], 'valores incorretos'\n",
    "print 'ok!'\n",
    "assert tokenize(\"Being at the_?\") == ['the_'], 'valores incorretos'\n",
    "print 'ok!'\n",
    "assert tokenize(quickbrownfox) == ['quick','brown','fox','jumps','lazy','dog'], 'valores incorretos'\n",
    "print 'ok!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1c) Tokenizando a base de dados**\n",
    "\n",
    "#### Vamos aplicar nosso tokenizador em nossa base de dados e contar o número total de tokens e criar uma função que conta quantas tokens existem em nossa base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "dataToToken = dataRDD.<COMPLETAR>\n",
    "\n",
    "def countTokens(textRDD):\n",
    "    \"\"\" Count and return the number of tokens\n",
    "    Args:\n",
    "        textRDD (RDD of tokens): RDD containing tokens for \n",
    "    Returns:\n",
    "        count: count of all tokens\n",
    "    \"\"\"\n",
    "    return (textRDD\n",
    "            .<COMPLETAR>\n",
    "            .<COMPLETAR>\n",
    "            ).collect()[0][1]\n",
    "\n",
    "totalTokens = countTokens(dataToToken)\n",
    "print 'There are %s tokens in the dataset' % totalTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Tokenizing the small datasets (1c)\n",
    "assert totalTokens == 80502,  'valor incorreto'\n",
    "print 'ok!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1d) Registro com maior número de tokens**\n",
    "\n",
    "#### Ordene o RDD e encontre o registro com maior número de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def findBiggestRecord(textRDD):\n",
    "    \"\"\" Find and return the record with the largest number of tokens\n",
    "    Args:\n",
    "        textRDD (RDD of (recordId, tokens)): input Pair Tuple of record ID and tokens\n",
    "    Returns:\n",
    "        list: a list of 1 Pair Tuple of record ID and tokens\n",
    "    \"\"\"\n",
    "    return (textRDD            \n",
    "            .<COMPLETAR>\n",
    "            ).take(1)\n",
    "\n",
    "biggestRecord = findBiggestRecord(dataRDD)\n",
    "print 'The record with ID \"%s\" has the most tokens (%s)' % (biggestRecord[0][0],\n",
    "                                                                   len(biggestRecord[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Amazon record with the most tokens (1d)\n",
    "assert 105156==biggestRecord[0][0], 'valor incorreto'\n",
    "print 'ok'\n",
    "assert 283==len(biggestRecord[0][1]), 'valor incorreto'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 2: Bag-of-Words Ponderados usando TF-IDF**\n",
    "\n",
    "#### Particularmente em textos grandes nem todos os tokens tem a mesma importância na descrição de um texto. Ponderações nos ajudam a mensurar a importância de cada elemento de um vetor. Uma heurística para medir a importância dos tokens é a chamada \"Term-Frequency/Inverse-Document-Frequency\", ou [TF-IDF][tfidf].\n",
    "\n",
    "#### **TF**\n",
    "#### TF diz que a importância de um token é proporcional a frequência com que ele aparece no documento. Ou seja, se um documento *d* contém 100 tokens  e o token *t* aparece em *d* por 5 vezes, então o peso TF de *t* em *d* é *5/100 = 1/20*. A intuição é de que se um token ocorre com mais frequência do que os outros tokens, ele deve ter um contexto mais importante.\n",
    "\n",
    "#### **IDF**\n",
    "#### IDF pondera os tokens proporcional ao inverso da frequência dele na base de dados. Ou seja, se dois documentos possuem um termo em comum que é frequente, essa comparação não terá um peso tão grande quanto dois documentos que compartilham um termo raro. IDF de um token *t*, em um conjunto de documentos, *U*, é calculado como:\n",
    "\n",
    "* #### Seja *N* a quantidade total de documentos em *U*\n",
    "* #### Encontre *n(t)*, o número de documentos em *U* que contém *t*\n",
    "* #### *IDF(t) = N/n(t)*.\n",
    "\n",
    "#### **TF-IDF**\n",
    "#### Finalmente, o peso TF-IDF é o produto entre TF e IDF de um termo.\n",
    "\n",
    "[tfidf]: https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2a) Implemente a função TF**\n",
    "\n",
    "#### Implemente `tf(tokens)` que recebe uma lista de tokens e retorna um dicionário Python [dictionary](https://docs.python.org/2/tutorial/datastructures.html#dictionaries) mapeando os tokens para pesos TF.\n",
    "\n",
    "#### Os passos que a função deve realizar:\n",
    "* #### Crie um dicionário vazio\n",
    "* #### Para cada token na lista `tokens`, conte 1 para cada ocorrência e some o token ao dicionário\n",
    "* #### Para cada token no dicionário, divida o total de ocorrência do token pelo total de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def tf(tokens):\n",
    "    \"\"\" Compute TF\n",
    "    Args:\n",
    "        tokens (list of str): input list of tokens from tokenize\n",
    "    Returns:\n",
    "        dictionary: a dictionary of tokens to its TF values\n",
    "    \"\"\"\n",
    "    freq = {}\n",
    "    if len(tokens)==0:\n",
    "        return {}\n",
    "    inc = <COMPLETAR>\n",
    "    for token in tokens:\n",
    "        freq[token] = <COMPLETAR>\n",
    "    return freq\n",
    "\n",
    "print tf(tokenize(quickbrownfox)) # Should give { 'quick': 0.1666 ... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Implement a TF function (2a)\n",
    "tf_test = tf(tokenize(quickbrownfox))\n",
    "assert tf_test == {'brown': 0.16666666666666666, 'lazy': 0.16666666666666666,'jumps': 0.16666666666666666, 'fox': 0.16666666666666666,\n",
    "                             'dog': 0.16666666666666666, 'quick': 0.16666666666666666},'valores incorretos'\n",
    "print 'ok'\n",
    "tf_test2 = tf(tokenize('one_ one_ two!'))\n",
    "assert tf_test2 == {'one_': 0.6666666666666666, 'two': 0.3333333333333333},'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2b) Implemente a função IDF**\n",
    "\n",
    "#### Implemente `idfs` que recebe um RDD de textos como parâmetro e calcula o peso IDF de cada token único. A função deve retornar uma RDD de tupla onde a chave é o token e o valor seu IDF correspondente.\n",
    "\n",
    "#### Lembre-se que o peso IDF de um token *t*, em um conjunto de documentos, *U*, é calculado da seguinte forma:\n",
    "* #### Calcule *N* como o total de registros em *U*.\n",
    "* #### Encontre *n(t)*, o número de documentos em *U* que contém *t*.\n",
    "* #### *IDF(t) = N/n(t)*.\n",
    "#### Os passos que você deve fazer é:\n",
    "* #### Calcule *N*. Isso pode ser feito utilizando um método próprio dos RDDs.\n",
    "* #### Crie um RDD contendo os tokens únicos de cada documento. Para cada documento, você deve incluir um token apenas uma vez, *mesmo que ele apareça mais de uma vez*.\n",
    "* #### Para cada um dos tokens únicos, conte quantas vezes ele aparece no documento e então compute o IDF para o token: *N/n(t)*\n",
    "#### Use seu `idfs` para calcular o IDF da sua base dataToToken.\n",
    "#### Quantos tokens únicos temos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def idfs(corpus):\n",
    "    \"\"\" Compute IDF\n",
    "    Args:\n",
    "        corpus (RDD): input corpus\n",
    "    Returns:\n",
    "        RDD: a RDD of (token, IDF value)\n",
    "    \"\"\"\n",
    "    N = corpus.count()\n",
    "    uniqueTokens = corpus.<COMPLETAR>\n",
    "    tokenCountPairTuple = uniqueTokens.<COMPLETAR>\n",
    "    tokenSumPairTuple = tokenCountPairTuple.<COMPLETAR>\n",
    "    return <COMPLETAR>\n",
    "\n",
    "idfsData = idfs(dataToToken)\n",
    "uniqueTokenCount = idfsData.count()\n",
    "\n",
    "print 'There are %s unique tokens in the small datasets.' % uniqueTokenCount\n",
    "print idfsData.takeOrdered(10, lambda s: s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert uniqueTokenCount==14909, 'valor incorreto!'\n",
    "print 'ok'\n",
    "tokenSmallestIdf = idfsData.takeOrdered(1, lambda s: s[1])[0]\n",
    "assert tokenSmallestIdf[0]==u'film', 'valor incorreto'\n",
    "print 'ok'\n",
    "assert abs(tokenSmallestIdf[1]-7.56028) < 0.00001, 'valor incorreto'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2d) Tokens com os menores IDF**\n",
    "#### Imprima os 11 tokens com menor IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smallIDFTokens = idfsData.takeOrdered(11, lambda s: s[1])\n",
    "print smallIDFTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2e) IDF Histograma**\n",
    "#### Plote um histograma dos valores de IDF.  Use a quantidade apropriada de buckets para o gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idf_values = idfsData.map(lambda s: s[1]).collect()\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.hist(idf_values, 50, log=True)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2f) Implemente a função TF-IDF**\n",
    "\n",
    "#### Use sua função `tf` para implementar uma função `tfidf(tokens,idfs)` que recebe uma lista de tokens de um documento e um dicionário Python de pesos IDF e retorna um dicionário Python mapeando cada token de `tokens` para seu peso total TF-IDF.\n",
    "\n",
    "#### Os passos de sua função devem ser:\n",
    "* #### Calcule o TF para `tokens`\n",
    "* #### Crie um dicionário Python onde cada token mapeia para o tf multiplicado pelo IDF desse token\n",
    "\n",
    "#### Use a função `tfidf` para computar os pesos do primeiro registro da RDD  dataToToken. Primeiro precisamos converter os IDFs dessa base para um dicionário Python. Para isso utilize a ação [`collectAsMap()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collectAsMap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def tfidf(tokens, idfs):\n",
    "    \"\"\" Compute TF-IDF\n",
    "    Args:\n",
    "        tokens (list of str): input list of tokens from tokenize\n",
    "        idfs (dictionary): record to IDF value\n",
    "    Returns:\n",
    "        dictionary: a dictionary of records to TF-IDF values\n",
    "    \"\"\"\n",
    "    tfs = <COMPLETAR>\n",
    "    tfIdfDict = <COMPLETAR>\n",
    "    return tfIdfDict\n",
    "\n",
    "firstDoc = dataToToken.take(1)[0][1]\n",
    "idfsWeights = idfsData.collectAsMap()\n",
    "rec_weights = tfidf(firstDoc, idfsWeights)\n",
    "\n",
    "print 'O primeiro documento tem tokens e pesos:\\n%s' % rec_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Implement a TF-IDF function (2f)\n",
    "assert rec_weights=={u'independent': 284.26666666666665, u'seeking': 177.66666666666666, u'quiet': 59.222222222222214, u'introspective': 473.7777777777777, u'entertaining': 15.44927536231884, u'worth': 18.70175438596491}, 'valores incorretos'\n",
    "print \"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 3: Similaridade do Cosseno**\n",
    "\n",
    "#### Nosso dicionário de TF-IDF pode ser visto como uma representação do texto em forma de um vetor esparso. Cada elemento do vetor pode ser nulo ou conter um valor numérico representando a importância daquele token para o documento.\n",
    "\n",
    "#### Uma métrica utilizada para esse tipo de representaçao é a **[similaridade do cosseno][cosine]** que calcula o cosseno do ângulo entre dois vetores para indicar quão próximose eles estão entre si, independente da escala:\n",
    "\n",
    "#### $$ a \\cdot b = \\| a \\| \\| b \\| \\cos \\theta $$\n",
    "#### $ a \\cdot b = \\sum a_i b_i $ é o produto interno de dois vetores, e $ \\|a\\| = \\sqrt{ \\sum a_i^2 } $ é a norma de $ a $.\n",
    "\n",
    "#### Então temos que a similaridade é:\n",
    "#### $$ similaridade = \\cos \\theta = \\frac{a \\cdot b}{\\|a\\| \\|b\\|} = \\frac{\\sum a_i b_i}{\\sqrt{\\sum a_i^2} \\sqrt{\\sum b_i^2}} $$\n",
    "\n",
    "#### Essa métrica indica que, se o ângulo entre dois documentos é pequeno, eles compartilham muitos tokens em comum, pois estão apontando mais ou menos para a mesma direção. Para esses casos, o cosseno do ângulo será alto. \n",
    "\n",
    "[cosine]: https://en.wikipedia.org/wiki/Cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(3a) Implemente os componentes de uma função `cosineSimilarity`**\n",
    "\n",
    "#### Use as funções `tokenize` e `tfidf`, e os pesos IDF da Parte 2 para extrair os tokens e atribuir pesos a eles.\n",
    "\n",
    "#### Os passos a serem realizados devem ser:\n",
    "* #### Defina uma função `dotprod` que recebe dois dicionários Python como parâmetro e calcula o produto interno deles, onde o produto interno é definido como a soma do produto dos valores para os tokens que aparecem em *ambos* os dicionários\n",
    "* #### Defina uma função `norm` que retorna a raíz quadrada do produto interno do dicionário com ele mesmo\n",
    "* #### Defina uma função `cossim` que retorna o produto interno entre dois dicionários dividido pelo produto das normas desses dicionários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "import math\n",
    "\n",
    "def dotprod(a, b):\n",
    "    \"\"\" Compute dot product\n",
    "    Args:\n",
    "        a (dictionary): first dictionary of record to value\n",
    "        b (dictionary): second dictionary of record to value\n",
    "    Returns:\n",
    "        dotProd: result of the dot product with the two input dictionaries\n",
    "    \"\"\"\n",
    "    return <COMPLETAR>\n",
    "\n",
    "def norm(a):\n",
    "    \"\"\" Compute square root of the dot product\n",
    "    Args:\n",
    "        a (dictionary): a dictionary of record to value\n",
    "    Returns:\n",
    "        norm: a dictionary of tokens to its TF values\n",
    "    \"\"\"\n",
    "    return <COMPLETAR>\n",
    "\n",
    "def cossim(a, b):\n",
    "    \"\"\" Compute cosine similarity\n",
    "    Args:\n",
    "        a (dictionary): first dictionary of record to value\n",
    "        b (dictionary): second dictionary of record to value\n",
    "    Returns:\n",
    "        cossim: dot product of two dictionaries divided by the norm of the first dictionary and\n",
    "                then by the norm of the second dictionary\n",
    "    \"\"\"\n",
    "    return <COMPLETAR>\n",
    "\n",
    "testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }\n",
    "testVec2 = {'foo': 1, 'bar': 0, 'baz': 20 }\n",
    "dp = dotprod(testVec1, testVec2)\n",
    "nm = norm(testVec1)\n",
    "print dp, nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Implement the components of a cosineSimilarity function (3a)\n",
    "assert dp==102, 'valor incorreto'\n",
    "print 'ok'\n",
    "assert abs(nm - 6.16441400297) < 0.0000001, 'valor incorreto'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(3b) Função `cosineSimilarity` **\n",
    "\n",
    "#### Implemente a função `cosineSimilarity(string1, string2, idfsDictionary)`  que recebe duas strings e um dicionário de pesos IDF e retorna a similaridade do cosseno para essas strings.\n",
    "\n",
    "#### Os passos a serem realizados:\n",
    "* #### Aplique sua função `tfidf` para a primeira e segunda strings tokenizadas (pela função `tokenize`) e usando o dicionário de pesos IDF\n",
    "* #### Aplique a função `cossim` retornando seu valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def cosineSimilarity(string1, string2, idfsDictionary):\n",
    "    \"\"\" Compute cosine similarity between two strings\n",
    "    Args:\n",
    "        string1 (str): first string\n",
    "        string2 (str): second string\n",
    "        idfsDictionary (dictionary): a dictionary of IDF values\n",
    "    Returns:\n",
    "        cossim: cosine similarity value\n",
    "    \"\"\"\n",
    "    w1 = <COMPLETAR>\n",
    "    w2 = <COMPLETAR>\n",
    "    return cossim(w1, w2)\n",
    "\n",
    "cossimreview = cosineSimilarity('this movie is good',\n",
    "                               'this movie is bad',\n",
    "                               idfsWeights)\n",
    "\n",
    "print cossimreview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Implement a cosineSimilarity function (3b)\n",
    "assert abs(cossimreview - 0.04446) < 0.0001, 'valor incorreto'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(3c) Similaridade entre documentos**\n",
    "\n",
    "#### Agora podemos finalmente calcular a similaridade entre os documentos de nossa base.\n",
    "\n",
    "#### Para gerar a matriz de distância entre cada documento vamos fazer os seguintes passos:\n",
    "\n",
    "* #### Crie uma RDD que é a combinação de cada registro da base dataRDD com ela mesma, com isso teremos uma RDD no formato `[ ( (id1, string1), (id2, string2) ),  ( (id1, string1), (id3, string3) ), ... ]`\n",
    "* #### Crie uma função que calcule a similaridade do cosseno para um dado registro dessa RDD combinada e utilizando uma variável broadcast para os pesos idfs\n",
    "* #### Crie uma variável broadcast com os pesos IDFs calculados\n",
    "* #### Aplique essa função na RDD\n",
    "\n",
    "#### NOTA: alguns bugs irão ocorrer nas funções tf e cosineSimilarity, vocês devem ler a mensagem de erro e corrigir tais códigos conforme necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "samplesRDD = dataRDD.sample(False, 0.1, 42).cache()\n",
    "\n",
    "crossRDD = (samplesRDD\n",
    "              .<COMPLETAR>\n",
    "              .cache())\n",
    "\n",
    "def computeSimilarity(record):\n",
    "    \"\"\" Compute similarity on a combination record\n",
    "    Args:\n",
    "        record: a pair, (rec1, rec2)\n",
    "    Returns:\n",
    "        pair: a pair, (rec1_id, rec2_id, cosine similarity value)\n",
    "    \"\"\"    \n",
    "    <COMPLETAR>\n",
    "    return (rec1_id, rec2_id, cs)\n",
    "\n",
    "idfsWeightsBroadcast = sc.broadcast(idfsWeights)\n",
    "similarities = (crossRDD\n",
    "                .map(computeSimilarity)\n",
    "                .cache())\n",
    "\n",
    "print similarities.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert similarities.take(1)[0][2]==1.0, 'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 4: k-NN escalável**\n",
    "\n",
    "#### Na parte anterior, o cálculo de similaridade entre dois documentos está limitado a uma complexidade quadrática, o que não é prática em bases de dados de tamanho moderado. Vamos implementar uma forma escalável de calcular as distâncias entre pares de documentos.\n",
    "\n",
    "### Índices invertidos\n",
    "\n",
    "#### Notem que boa parte dos documentos tem similaridade igual a zero, ou seja, não possuem tokens em comum. Para resolver esse problema podemos utilizar a estrutura de dados chamada [**índice invertido**](https://en.wikipedia.org/wiki/Inverted_index) que mapeia cada token na base de dados a lista de documentos que os contém. Então ao invés de compararmos todos os pares de registros para verificar se eles contém tokens em comum, nós iremos nos restringir apenas aos pares de documentos que tem algo em comum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4a) Tokenize a base de dados completa**\n",
    "#### Tokenize a base dataRDD para pré-processar os tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "tokenRDD = dataRDD.<COMPLETAR>\n",
    "print 'A base de dados contém %s itens' % (tokenRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert tokenRDD.count() == 8528, 'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4b) Calcule os TF-IDFs para a base de dados **\n",
    "#### Vamos utilizar as funções já criadas e a variável de broadcast definida anteriormente (`idfsWeightsBroadcast`)\n",
    "#### Os passos a serem feitos:\n",
    "* #### Crie um novo `dataTFIDFRDD` primeiro mapeando os tokens de tokenRDD para os pesos tf\n",
    "* #### Em seguida, aplique os pesos idf para cada token, gerando um dicionário (token, tfidf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "dataTFIDFRDD = (tokenRDD\n",
    "                .<COMPLETAR>\n",
    "                .<COMPLETAR>\n",
    "               )\n",
    "\n",
    "print 'There are %s weights .' % (dataTFIDFRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert dataTFIDFRDD.count()==8528, 'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4c) Pré-calculando as normas para os pesos da base**\n",
    "\n",
    "* #### Crie uma RDD de tuplas em que temos (id, norm(tokens))\n",
    "* #### Converta essa RDD em uma variável de broadcast como um dicionário de normas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "dataNorms = dataTFIDFRDD.map(lambda rec: (rec[0],norm(rec[1])))\n",
    "dataNormsBroadcast = sc.broadcast(dataNorms.collectAsMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4d) Crie um índice invertido para a base completa**\n",
    "\n",
    "* #### Crie uma função `invert` que recebe um registro da base e gera uma lista de tuplas (token,(id, tfidf)) para cada token do registro. Lembre-se que o valor do RDD de tuplas é um dicionário com chaves iguais aos tokens e valores igual ao peso TF-IDF.\n",
    "\n",
    "* #### Use essa função para converter nossa RDD dataTFIDFRDD para uma RDD de índices invertidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def invert(record):\n",
    "    \"\"\" Invert (ID, tokens) to a list of (token, ID)\n",
    "    Args:\n",
    "        record: a pair, (ID, token vector)\n",
    "    Returns:\n",
    "        pairs: a list of pairs of token to ID\n",
    "    \"\"\"\n",
    "    <COMPLETAR>\n",
    "    return (pairs)\n",
    "\n",
    "dataInvPairsRDD = (dataTFIDFRDD\n",
    "                    .<COMPLETAR>\n",
    "                    .cache())\n",
    "\n",
    "print 'There are %s inverted pairs.' % (dataInvPairsRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert dataInvPairsRDD.count() == 79658, 'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4e) Identifique tokens em comum nos registros da base de dados**\n",
    "\n",
    "* #### Usando o índice invertido, agrupe a base pela chave (token) e aplique a função `genList` que deve gerar uma lista de tuplas (token, ((doc1,tfidf),(doc2,tfidf)) de todos os pares de documentos com essa token em comum exceto nos casos `doc1==doc2`.\n",
    "* #### Em seguida, gere tuplas do tipo ( (doc1, doc2), tfidf1*tfidf2/(norma1*norma2) ) a reduza para realizar a somatória desses valores sob a mesma chave.\n",
    "\n",
    "#### Dessa forma teremos os registros de pares de documentos que possuem similaridade não nula com sua similaridade calculada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "from itertools import product\n",
    "\n",
    "def genList(record):\n",
    "    \"\"\" generate a list of tuple of documents and a token\n",
    "    Args:\n",
    "        record: a pair, (token, [list of documents with tfidf])\n",
    "    Returns:\n",
    "        list: [( tuple, token)]\n",
    "    \"\"\"\n",
    "    <COMPLETAR>\n",
    "    return <COMPLETAR>\n",
    "\n",
    "def sumTFIDF(record):\n",
    "    \"\"\" Multiply the tfidf of both documents, to a given term and divide by the norm.\n",
    "    Args:\n",
    "        record: a pair, (((doc1, tfidf), (doc2, tfidf)), token)\n",
    "    Returns:\n",
    "        pair: ((ID, URL), tfidf)\n",
    "    \"\"\"   \n",
    "    <COMPLETAR>\n",
    "    return (key, value)\n",
    "\n",
    "commonTokens = (dataInvPairsRDD\n",
    "                .<COMPLETAR>\n",
    "                .<COMPLETAR>\n",
    "                .<COMPLETAR>\n",
    "                .<COMPLETAR>\n",
    "                .cache()\n",
    "                )\n",
    "\n",
    "#print 'Found %d common tokens' % commonTokens.count()\n",
    "print commonTokens.take(2)\n",
    "print commonTokens.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert commonTokens.count()==5620224, 'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4f) k-NN**\n",
    "\n",
    "#### Vamos gerar agora a lista dos *k* documentos mais similares de cada documento.\n",
    "\n",
    "* #### Gere uma RDD partindo da `commonTokens` de tal forma a ter ( id1, (id2, sim) )\n",
    "* #### Agrupe pela chave e transforme a RDD para ( id1, [ (id,sim) ] ) onde a lista deve ter k elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def genklist(rec,k):\n",
    "    \"\"\" Generate the list of the k most similar documents to the key\n",
    "    Args:\n",
    "        record: a pair, (doc, [(doc,sim)])\n",
    "        k: number of most similar elements\n",
    "    Returns:\n",
    "        pair: (doc, [(doc,sim)])\n",
    "    \"\"\"\n",
    "    <COMPLETAR>\n",
    "    return (key, docs[:k])\n",
    "    \n",
    "def knn(simRDD, k):\n",
    "    \"\"\" Generate the knn RDD for a given RDD.\n",
    "    Args:\n",
    "        simRDD: RDD of ( (doc1,doc2), sim)\n",
    "        k: number of most similar elements\n",
    "    Returns:\n",
    "        RDD: RDD of ( doc1, [docs, sims])\n",
    "    \"\"\"\n",
    "\n",
    "    ksimRDD = (simRDD\n",
    "               .<COMPLETAR>\n",
    "               .<COMPLETAR>\n",
    "               .<COMPLETAR>\n",
    "              )\n",
    "    return ksimRDD\n",
    "\n",
    "ksimReviewsRDD = knn(commonTokens, 3)\n",
    "ksimReviewsRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dataRDD.filter(lambda x: x[0] in [8198,12615,30826,46764]).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
