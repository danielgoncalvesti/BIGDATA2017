{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CMCC](http://cmcc.ufabc.edu.br/images/logo_site.jpg)\n",
    "\n",
    "# **Representando atributos textuais por quantização**\n",
    "\n",
    "\n",
    "### Faça upload do arquivo moviereviews.tsv (no site do curso) no mesmo diretório desse notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4999 lines\n",
      "Sample line: (1, u'\"When I heard about \\\\\"Hammerhead\\\\\" being released on DVD and finally found it at my local DVD store, I thought \\\\\"well, just another cheap monster movie from Nu Image\\\\\". Those guys around Boaz Davidson and Avi Lerner produced cheap but very entertaining B - Pictures in the past few months but also some very disappointing movies. So I didn\\'t expect much, especially after having watched the rather disappointing \\\\\"Shark Zone\\\\\" just a few days before. But \\\\\"Hammerhead\\\\\" turned out to be an excellent revival of the 1950s monster movies. We have a mad scientist, a group of people in a dangerous situation, screaming women and damsels in distress, man-eating plants and of course we have the creature, a huge mutant mix between a man and a hammerhead shark. Everything you need for an entertaining monster movie. The only thing missing are graphic sex scenes and nudity which you expect in movies of this kind, but since the movie was made for TV it\\'s understandable why these scenes are missing. And it doesn\\'t matter anyway cause \\\\\"Hammerhead\\\\\" is action and horror entertainment at it\\'s best. There are two reasons why I gave it seven out of ten points, though: First of all, the monster isn\\'t seen very often and the showdown with the destruction of the creature is too fast and poorly done, and secondly, William Forsythe just isn\\'t the right guy for the \\\\\"hero\\\\\" part and for falling in love with gorgeous Hunter Tylo. Other than that, I can highly recommend this movie for any monster movie fan out there. Grab yourselves a cool drink and some popcorn, watch this movie and have fun. Jasper P. Morgan\"')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def parseRDD(point):\n",
    "    \"\"\" Parser for the current dataset. It receives a data point and return\n",
    "        a sentence (third field).\n",
    "    Args:\n",
    "        point (str): input data point\n",
    "    Returns:\n",
    "        str: a string\n",
    "    \"\"\"    \n",
    "    data = point.split('\\t')\n",
    "    return (int(data[1]),data[2])\n",
    "\n",
    "def notempty(point):\n",
    "    \"\"\" Returns whether the point string is not empty\n",
    "    Args:\n",
    "        point (str): input string\n",
    "    Returns:\n",
    "        bool: True if it is not empty\n",
    "    \"\"\"   \n",
    "    return len(point[1])>0\n",
    "\n",
    "filename = os.path.join(\"moviereviews.tsv\")\n",
    "rawRDD = sc.textFile(filename,100)\n",
    "header = rawRDD.take(1)[0]\n",
    "\n",
    "dataRDD = (rawRDD\n",
    "           #.sample(False, 0.1, seed=42)\n",
    "           .filter(lambda x: x!=header)\n",
    "           .map(parseRDD)\n",
    "           .filter(notempty)\n",
    "           #.sample( False, 0.1, 42 )\n",
    "           )\n",
    "\n",
    "print 'Read {} lines'.format(dataRDD.count())\n",
    "print 'Sample line: {}'.format(dataRDD.takeSample(False, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'stuff', u'going', u'moment', u'started', u'listening', u'music', u'watching', u'odd', u'documentary', u'watched', u'wiz', u'watched', u'moonwalker', u'maybe', u'want', u'get', u'certain', u'insight', u'guy', u'thought', u'really', u'cool', u'eighties', u'maybe', u'make', u'mind', u'whether', u'guilty', u'innocent', u'moonwalker', u'part', u'biography', u'part', u'feature', u'film', u'remember', u'going', u'see', u'cinema', u'originally', u'released', u'subtle', u'messages', u'feeling', u'towards', u'press', u'also', u'obvious', u'message', u'drugs', u'bad', u'kay', u'visually', u'impressive', u'course', u'michael', u'jackson', u'unless', u'remotely', u'like', u'anyway', u'going', u'hate', u'find', u'boring', u'may', u'call', u'egotist', u'consenting', u'making', u'movie', u'fans', u'would', u'say', u'made', u'fans', u'true', u'really', u'nice', u'actual', u'feature', u'film', u'bit', u'finally', u'starts', u'minutes', u'excluding', u'smooth', u'criminal', u'sequence', u'joe', u'pesci', u'convincing', u'psychopathic', u'powerful', u'drug', u'lord', u'wants', u'dead', u'bad', u'beyond', u'overheard', u'plans', u'nah', u'joe', u'pesci', u'character', u'ranted', u'wanted', u'people', u'know', u'supplying', u'drugs', u'etc', u'dunno', u'maybe', u'hates', u'music', u'lots', u'cool', u'things', u'like', u'turning', u'car', u'robot', u'whole', u'speed', u'demon', u'sequence', u'also', u'director', u'must', u'patience', u'saint', u'came', u'filming', u'kiddy', u'bad', u'sequence', u'usually', u'directors', u'hate', u'working', u'one', u'kid', u'let', u'alone', u'whole', u'bunch', u'performing', u'complex', u'dance', u'scene', u'bottom', u'line', u'movie', u'people', u'like', u'one', u'level', u'another', u'think', u'people', u'stay', u'away', u'try', u'give', u'wholesome', u'message', u'ironically', u'bestest', u'buddy', u'movie', u'girl', u'michael', u'jackson', u'truly', u'one', u'talented', u'people', u'ever', u'grace', u'planet', u'guilty', u'well', u'attention', u'gave', u'subject', u'hmmm', u'well', u'know', u'people', u'different', u'behind', u'closed', u'doors', u'know', u'fact', u'either', u'extremely', u'nice', u'stupid', u'guy', u'one', u'sickest', u'liars', u'hope', u'latter']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "stopfile = os.path.join(\"Data\",\"Aula04\",\"stopwords.txt\")\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    return filter(lambda x: len(x)>2 and x not in stopwords,re.split(split_regex,string.lower()))\n",
    "\n",
    "wordsRDD = dataRDD.map(lambda x: tokenize(x[1]))\n",
    "\n",
    "print wordsRDD.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.430164247751,0.0665539652109,-0.30026063323,0.304091185331,0.192403480411]\n",
      "[(u'thrills', 0.99504232406616211), (u'dynamite', 0.98769634962081909)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "model = Word2Vec().setVectorSize(5).setSeed(42).fit(wordsRDD)\n",
    "\n",
    "print model.transform(u'entertaining')\n",
    "print model.findSynonyms(u'entertaining', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12540 tokens únicos\n"
     ]
    }
   ],
   "source": [
    "uniqueWords = (wordsRDD\n",
    "               .flatMap(lambda x: [(w,1) for w in x])\n",
    "               .reduceByKey(lambda x,y: x+y)\n",
    "               .filter(lambda x: x[1]>=5)\n",
    "               .map(lambda x: x[0])\n",
    "               .collect()\n",
    "               )\n",
    "\n",
    "print '{} tokens únicos'.format(len(uniqueWords))\n",
    "\n",
    "w2v = {}\n",
    "for w in uniqueWords:\n",
    "    w2v[w] = np.array(model.transform(w))\n",
    "w2vb = sc.broadcast(w2v)       \n",
    "\n",
    "vectorsRDD = (wordsRDD\n",
    "              .map(lambda x: np.array([w2vb.value[w] for w in x if w in w2vb.value]))\n",
    "             )\n",
    "recs = vectorsRDD.take(2)\n",
    "firstRec, secondRec = recs[0], recs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from  pyspark.mllib.clustering import KMeans\n",
    "\n",
    "ncluster = 200\n",
    "\n",
    "vectors2RDD = sc.parallelize(np.array(w2v.values()),1)\n",
    "\n",
    "modelK = KMeans.train(vectors2RDD, ncluster, seed=42 )\n",
    "\n",
    "clustersRDD = vectors2RDD.map(lambda x: modelK.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(1.0, [3.0,1.0,0.0,4.0,3.0,1.0,0.0,1.0,5.0,4.0,6.0,9.0,0.0,1.0,7.0,4.0,2.0,3.0,6.0,9.0,0.0,7.0,5.0,1.0,7.0,5.0,0.0,33.0,5.0,0.0,9.0,0.0,5.0,2.0,1.0,6.0,1.0,9.0,3.0,6.0,0.0,3.0,0.0,2.0,1.0,4.0,8.0,0.0,3.0,0.0])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def quantizador(point, model, k, w2v):\n",
    "    key = point[0]\n",
    "    words = tokenize(point[1])\n",
    "    matrix = np.array( [w2v[w] for w in words if w in w2v] )\n",
    "    features = np.zeros(k)\n",
    "    for v in matrix:\n",
    "        c = model.predict(v)\n",
    "        features[c] += 1\n",
    "    return LabeledPoint(key, features)\n",
    "    \n",
    "quantRDD = (dataRDD\n",
    "            .map(lambda x: quantizador(x, modelK, ncluster, w2v))\n",
    "            .filter(lambda x: x.label!=2)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(1.0, [3.0,1.0,0.0,4.0,3.0,1.0,0.0,1.0,5.0,4.0,6.0,9.0,0.0,1.0,7.0,4.0,2.0,3.0,6.0,9.0,0.0,7.0,5.0,1.0,7.0,5.0,0.0,33.0,5.0,0.0,9.0,0.0,5.0,2.0,1.0,6.0,1.0,9.0,3.0,6.0,0.0,3.0,0.0,2.0,1.0,4.0,8.0,0.0,3.0,0.0])]\n"
     ]
    }
   ],
   "source": [
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "quantTrainData, quantValidationData, quantTestData = quantRDD.randomSplit(weights, seed)\n",
    "# Cache the data\n",
    "quantTrainData.cache()\n",
    "quantValidationData.cache()\n",
    "quantTestData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.classification import SVMWithSGD, NaiveBayes\n",
    "\n",
    "def calcAccuracy( predsAndVals ):\n",
    "    return predsAndVals.map(lambda x: x[0]==x[1]).mean()\n",
    "\n",
    "# fixed hyperparameters\n",
    "numIters = 200\n",
    "regParam = 0.5\n",
    "regType = 'l2'\n",
    "includeIntercept = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Validation Accuracy = 0.642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelLR = LogisticRegressionWithLBFGS.train(quantTrainData, iterations=numIters, \n",
    "                                        regParam=regParam, regType=regType, intercept=includeIntercept)\n",
    "labelsAndPreds = quantValidationData.map(lambda x: (modelLR.predict(x.features),x.label))\n",
    "\n",
    "accValLR = calcAccuracy(labelsAndPreds)\n",
    "print  ('LR Validation Accuracy = {0:.3f}\\n').format(accValLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projeto em Grupo\n",
    "\n",
    "#### Aplique o seu algoritmo de clusterização no RDD `vectors2RDD` com `ncluster` grupos.\n",
    "#### Aplique a função quantizador utilizando o seu modelo de clusterização na base dataRDD conforme código exemplo:\n",
    "\n",
    "```\n",
    "quantRDD = (dataRDD\n",
    "            .map(lambda x: quantizador(x, modelK, ncluster, w2v))\n",
    "            .filter(lambda x: x.label!=2)\n",
    "            )\n",
    "```\n",
    "\n",
    "Note que `modelK` precisa ter um método `predict` que aloca um objeto a um cluster. Em seguida aplique a base de dados resultante na regressão logística conforme exemplo acima e calcule a acurácia. O valor deve ser parecido com o obtido acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
