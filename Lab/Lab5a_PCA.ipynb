{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CMCC](http://cmcc.ufabc.edu.br/images/logo_site.jpg)\n",
    "\n",
    "# **Principal Component Analysis**\n",
    "\n",
    "#### Esse notebook retoma o assunto de análise exploratória através do conceito de aprendizado não-supervisionado conhecido como principal component analysis (PCA). Usaremos o dataset [Million Song Dataset](http://labrosa.ee.columbia.edu/millionsong/) do repositório [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD) utilizado no Lab4a. \n",
    "\n",
    "#### ** Neste notebook: **\n",
    "+  ####*Parte 1:* Passo a passo do PCA em um dataset artificial\n",
    " + ####*Visualização 1:* Gaussianas bidimensionais\n",
    "+  ####*Parte 2:* Função PCA para aplicar em um RDD\n",
    " + ####*Visualização 2:* Projeção do PCA\n",
    "+  ####*Parte 3:* Aplicação do PCA na base Million Song, com aplicação de Regressão Linear na base projetada\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### **Parte 1: Passo a passo do PCA em um dataset artificial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualização 1: Gaussianas bidimensionais**\n",
    "\n",
    "#### Principal Component Analysis, ou PCA, é uma estratégia para redução de dimensionalidade. Para entender como o PCA funciona, vamos utilizar uma base de dados gerada artificialmente através de uma [distribuição Gaussiana bidimensional](http://en.wikipedia.org/wiki/Multivariate_normal_distribution).  Essa distribuição tem como parâmetros a média e variância de cada dimensão, assim como a covariância entre as dimensões.\n",
    " \n",
    "#### Para nossos experimentos vamos definir a média para cada dimensão como 50 e a variância como 1. E testaremos dois valores para covariância: 0 e 0.9. Quando a covariância é zero significa que as variáveis não possuem correlação, e os dados são esféricos. Quando setamos a covariância para 0.9, significa que as duas dimenões possuem correlação positiva e forte, ou seja, quando uma variável aumenta seu valor a outra tende a aumentar também. Quando temos uma correlação forte significa que podemos representar duas ou mais variáveis como apenas uma, veremos que o PCA funciona muito bem para esses casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def preparePlot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n",
    "                gridWidth=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hideLabels: axis.set_ticklabels([])\n",
    "    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax\n",
    "\n",
    "def create2DGaussian(mn, sigma, cov, n):\n",
    "    \"\"\"Randomly sample points from a two-dimensional Gaussian distribution\"\"\"\n",
    "    np.random.seed(142)\n",
    "    return np.random.multivariate_normal(np.array([mn, mn]), np.array([[sigma, cov], [cov, sigma]]), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataRandom = create2DGaussian(mn=50, sigma=1, cov=0, n=100)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45, 54.5), ax.set_ylim(45, 54.5)\n",
    "plt.scatter(dataRandom[:,0], dataRandom[:,1], s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataCorrelated = create2DGaussian(mn=50, sigma=1, cov=.9, n=100)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
    "plt.scatter(dataCorrelated[:,0], dataCorrelated[:,1], s=14**2, c='#d6ebf2',\n",
    "            edgecolors='#8cbfd0', alpha=0.75)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1a) Interpretando o PCA**\n",
    "\n",
    "#### PCA pode ser interpretado como a identificação das direções em que os dados variam mais. No primeiro passo do PCA, precisamos centralizar nossos dadods. O primeiro passo requer que calculemos a média de cada atributo (coluna) da base de dados. Em seguida, para cada observação (linha), modifique os valores dos atributos pela média correspondente. Dessa forma teremos uma base de dados em que cada atributo tem média zero.\n",
    "\n",
    "#### Note que `correlatedData` é uma RDD de NumPy arrays. Isso permite que façamos algumas operações mais facilmente. Por exemplo, se utilizarmos a transformação `sum()` o PySpark irá fazer a soma dos valores de cada coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "correlatedData = sc.parallelize(dataCorrelated)\n",
    "\n",
    "meanCorrelated = correlatedData.<COMPLETAR>\n",
    "correlatedDataZeroMean = correlatedData.<COMPLETAR>\n",
    "\n",
    "print meanCorrelated\n",
    "print correlatedData.take(1)\n",
    "print correlatedDataZeroMean.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Interpreting PCA (1a)\n",
    "from test_helper import Test\n",
    "Test.assertTrue(np.allclose(meanCorrelated, [49.95739037, 49.97180477]),\n",
    "                'incorrect value for meanCorrelated')\n",
    "Test.assertTrue(np.allclose(correlatedDataZeroMean.take(1)[0], [-0.28561917, 0.10351492]),\n",
    "                'incorrect value for correlatedDataZeroMean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1b) Matriz de Covariância**\n",
    "\n",
    "#### Vamos calcular a matriz de covariância de nossos dados. Se definirmos $\\scriptsize \\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ como a matriz de dados centrada em zero, então a matriz de covariância é definida como: $$ \\mathbf{C}_{\\mathbf X} = \\frac{1}{n} \\mathbf{X}^\\top \\mathbf{X} \\,.$$  Essa matriz pode ser calculada computando o produto externo de cada linha com ela mesma, em seguida realizando a somatória das matrizes  resultantes e, finalmente, divindo pelo total de objetos na base de dados. Os dados são bidimensionais, então a matriz de covariância é uma matriz 2x2.\n",
    " \n",
    "#### Note que [np.outer()](http://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html) pode ser utilizado para calcular o produto externo de duas NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "# Compute the covariance matrix using outer products and correlatedDataZeroMean\n",
    "correlatedCov = (correlatedDataZeroMean\n",
    "                 .<COMPLETAR>\n",
    "                 .<COMPLETAR>\n",
    "                 )/correlatedDataZeroMean.count()\n",
    "print correlatedCov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Sample covariance matrix (1b)\n",
    "covResult = [[ 0.99558386,  0.90148989], [0.90148989, 1.08607497]]\n",
    "Test.assertTrue(np.allclose(covResult, correlatedCov), 'incorrect value for correlatedCov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1c) Função de Covariância**\n",
    "\n",
    "#### Utilizando as expressões do exercício anterior, faça uma função que calcule a matriz de covariância de uma RDD arbitrária. Note que deve-se centralizar os dados para que eles tenha média zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def estimateCovariance(data):\n",
    "    \"\"\"Compute the covariance matrix for a given rdd.\n",
    "\n",
    "    Note:\n",
    "        The multi-dimensional covariance array should be calculated using outer products.  Don't\n",
    "        forget to normalize the data by first subtracting the mean.\n",
    "\n",
    "    Args:\n",
    "        data (RDD of np.ndarray):  An `RDD` consisting of NumPy arrays.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the\n",
    "            length of the arrays in the input `RDD`.\n",
    "    \"\"\"\n",
    "    meanVal = data.<COMPLETAR>\n",
    "    return (data\n",
    "            .<COMPLETAR>\n",
    "            .<COMPLETAR>\n",
    "            .<COMPLETAR>\n",
    "           )/data.count()\n",
    "\n",
    "correlatedCovAuto= estimateCovariance(correlatedData)\n",
    "print correlatedCovAuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Covariance function (1c)\n",
    "correctCov = [[ 0.99558386,  0.90148989], [0.90148989, 1.08607497]]\n",
    "Test.assertTrue(np.allclose(correctCov, correlatedCovAuto),\n",
    "                'incorrect value for correlatedCovAuto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1d) Autovalores e Autovetores**\n",
    "\n",
    "#### Agora que temos a matriz de covariância, podemos usá-la para encontrar as direções de maior variância nos dados. Os autovalores e autovetores nos trazem tal informação. Os $\\scriptsize d $ autovetores da matriz de covariância nos dá as direções de maior variância e são chamadas de componentes principais. Os autovalores associados são as variâncias nessas direções. Particularmente, o atuovetor correspondente ao maior autovalor é a direção de máxima variância. O cálculo dos autovalores e autovetores da matriz de covariância tem uma complexidade aproximademente $O(d^3)$ para uma matriz d x d. Quando nosso $d$ é suficientemente pequeno, podemos computar os autovalores e autovetores localmente.\n",
    " \n",
    "#### Use a função [eigh](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html) da biblioteca `numpy.linalg` para calcular os autovalores e autovetores. Em seguida, ordene os autovetores baseado nos autovalores, do maior para o menor, gerando uma matriz em que os autovetores são as colunas (e a primeira coluna é o maior autovetor).\n",
    "\n",
    "#### Note que a função [np.argsort](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy-argsort) pode ser usada para obter a ordem dos índices dos elementos em ordem crescente ou decrescente.  Finalmente coloque o maior autovetor na variável `topComponent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "from numpy.linalg import eigh\n",
    "\n",
    "# Calculate the eigenvalues and eigenvectors from correlatedCovAuto\n",
    "eigVals, eigVecs = <COMPLETAR>\n",
    "print 'eigenvalues: {0}'.format(eigVals)\n",
    "print '\\neigenvectors: \\n{0}'.format(eigVecs)\n",
    "\n",
    "# Use np.argsort to find the top eigenvector based on the largest eigenvalue\n",
    "inds = <COMPLETAR>\n",
    "topComponent = <COMPLETAR>\n",
    "print '\\ntop principal component: {0}'.format(topComponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Eigendecomposition (1d)\n",
    "def checkBasis(vectors, correct):\n",
    "    return np.allclose(vectors, correct) or np.allclose(np.negative(vectors), correct)\n",
    "Test.assertTrue(checkBasis(topComponent, [0.68915649, 0.72461254]),\n",
    "                'incorrect value for topComponent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1e) PCA scores**\n",
    "\n",
    "#### Nós calculamos os principais componentes para uma base de dados não-esférica. Agora vamos usar este componente principal para derivar uma representação unidimensional dos dados originais. Para calcular a representação compact, que é chamada PCA scores, calcule o produto interno entre cada objeto na base original e o componente principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "# Use the topComponent and the data from correlatedData to generate PCA scores\n",
    "correlatedDataScores = correlatedData.<COMPLETAR>\n",
    "print 'one-dimensional data (first three):\\n{0}'.format(np.asarray(correlatedDataScores.take(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST PCA Scores (1e)\n",
    "firstThree = [70.51682806, 69.30622356, 71.13588168]\n",
    "Test.assertTrue(checkBasis(correlatedDataScores.take(3), firstThree),\n",
    "                'incorrect value for correlatedDataScores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2: Função PCA para aplicar em um RDD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2a) Função PCA**\n",
    "\n",
    "#### Agora temos todos os ingredientes para calcular uma função PCA. Nossa função deve ser genérica para calcular os $k$ componentes principais e PCA scores. Escreva a função genérica `pca` e teste utilizando `correlatedData` e $\\scriptsize k = 2$. Dica: Use os resultados da Parte (1c), Parte (1d), e Parte (1e).\n",
    " \n",
    "####Nota: Conforme discutido anteriormente, essa implementação é eficiente enquanto $\\scriptsize d $ é pequeno, mas algoritmos distribuídos mais eficientes existem para $\\scriptsize d $ grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def pca(data, k=2):\n",
    "    \"\"\"Computes the top `k` principal components, corresponding scores, and all eigenvalues.\n",
    "\n",
    "    Note:\n",
    "        All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n",
    "        each eigenvectors as a column.  This function should also return eigenvectors as columns.\n",
    "\n",
    "    Args:\n",
    "        data (RDD of np.ndarray): An `RDD` consisting of NumPy arrays.\n",
    "        k (int): The number of principal components to return.\n",
    "\n",
    "    Returns:\n",
    "        tuple of (np.ndarray, RDD of np.ndarray, np.ndarray): A tuple of (eigenvectors, `RDD` of\n",
    "            scores, eigenvalues).  Eigenvectors is a multi-dimensional array where the number of\n",
    "            rows equals the length of the arrays in the input `RDD` and the number of columns equals\n",
    "            `k`.  The `RDD` of scores has the same number of rows as `data` and consists of arrays\n",
    "            of length `k`.  Eigenvalues is an array of length d (the number of features).\n",
    "    \"\"\"\n",
    "    <COMPLETAR>\n",
    "\n",
    "    # Return the `k` principal components, `k` scores, and all eigenvalues\n",
    "    return (vecs, data.map(lambda x: x.dot(vecs)), vals)\n",
    "\n",
    "# Run pca on correlatedData with k = 2\n",
    "topComponentsCorrelated, correlatedDataScoresAuto, eigenvaluesCorrelated = pca(correlatedData, 2)\n",
    "\n",
    "# Note that the 1st principal component is in the first column\n",
    "print 'topComponentsCorrelated: \\n{0}'.format(topComponentsCorrelated)\n",
    "print ('\\ncorrelatedDataScoresAuto (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, correlatedDataScoresAuto.take(3)))))\n",
    "print '\\neigenvaluesCorrelated: \\n{0}'.format(eigenvaluesCorrelated)\n",
    "\n",
    "# Create a higher dimensional test set\n",
    "pcaTestData = sc.parallelize([np.arange(x, x + 4) for x in np.arange(0, 20, 4)])\n",
    "componentsTest, testScores, eigenvaluesTest = pca(pcaTestData, 3)\n",
    "\n",
    "print '\\npcaTestData: \\n{0}'.format(np.array(pcaTestData.collect()))\n",
    "print '\\ncomponentsTest: \\n{0}'.format(componentsTest)\n",
    "print ('\\ntestScores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, testScores.take(3)))))\n",
    "print '\\neigenvaluesTest: \\n{0}'.format(eigenvaluesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST PCA Function (2a)\n",
    "Test.assertTrue(checkBasis(topComponentsCorrelated.T,\n",
    "                           [[0.68915649,  0.72461254], [-0.72461254, 0.68915649]]),\n",
    "                'incorrect value for topComponentsCorrelated')\n",
    "firstThreeCorrelated = [[70.51682806, 69.30622356, 71.13588168], [1.48305648, 1.5888655, 1.86710679]]\n",
    "Test.assertTrue(np.allclose(firstThreeCorrelated,\n",
    "                            np.vstack(np.abs(correlatedDataScoresAuto.take(3))).T),\n",
    "                'incorrect value for firstThreeCorrelated')\n",
    "Test.assertTrue(np.allclose(eigenvaluesCorrelated, [1.94345403, 0.13820481]),\n",
    "                           'incorrect values for eigenvaluesCorrelated')\n",
    "topComponentsCorrelatedK1, correlatedDataScoresK1, eigenvaluesCorrelatedK1 = pca(correlatedData, 1)\n",
    "\n",
    "Test.assertTrue(checkBasis(topComponentsCorrelatedK1.T, [0.68915649,  0.72461254]),\n",
    "               'incorrect value for components when k=1')\n",
    "Test.assertTrue(np.allclose([70.51682806, 69.30622356, 71.13588168],\n",
    "                            np.vstack(np.abs(correlatedDataScoresK1.take(3))).T),\n",
    "                'incorrect value for scores when k=1')\n",
    "Test.assertTrue(np.allclose(eigenvaluesCorrelatedK1, [1.94345403, 0.13820481]),\n",
    "                           'incorrect values for eigenvalues when k=1')\n",
    "Test.assertTrue(checkBasis(componentsTest.T[0], [ .5, .5, .5, .5]),\n",
    "                'incorrect value for componentsTest')\n",
    "Test.assertTrue(np.allclose(np.abs(testScores.first()[0]), 3.),\n",
    "                'incorrect value for testScores')\n",
    "Test.assertTrue(np.allclose(eigenvaluesTest, [ 128, 0, 0, 0 ]), 'incorrect value for eigenvaluesTest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2b) PCA em `dataRandom`**\n",
    "\n",
    "#### Agora, use a função PCA para encontrar os dois componentes principais da base esférica `dataRandom`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "randomData = sc.parallelize(dataRandom)\n",
    "\n",
    "# Use pca on randomData\n",
    "topComponentsRandom, randomDataScoresAuto, eigenvaluesRandom = <COMPLETAR>\n",
    "\n",
    "print 'topComponentsRandom: \\n{0}'.format(topComponentsRandom)\n",
    "print ('\\nrandomDataScoresAuto (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, randomDataScoresAuto.take(3)))))\n",
    "print '\\neigenvaluesRandom: \\n{0}'.format(eigenvaluesRandom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST PCA on `dataRandom` (2b)\n",
    "Test.assertTrue(checkBasis(topComponentsRandom.T,\n",
    "                           [[-0.2522559 ,  0.96766056], [-0.96766056,  -0.2522559]]),\n",
    "                'incorrect value for topComponentsRandom')\n",
    "firstThreeRandom = [[36.61068572,  35.97314295,  35.59836628],\n",
    "                    [61.3489929 ,  62.08813671,  60.61390415]]\n",
    "Test.assertTrue(np.allclose(firstThreeRandom, np.vstack(np.abs(randomDataScoresAuto.take(3))).T),\n",
    "                'incorrect value for randomDataScoresAuto')\n",
    "Test.assertTrue(np.allclose(eigenvaluesRandom, [1.4204546, 0.99521397]),\n",
    "                            'incorrect value for eigenvaluesRandom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualização 2: Projeção do PCA**\n",
    "\n",
    "#### Vamos ver o gráfico dos dados originais e a reconstrução unidimensional usando o componente principal para ver como a solução do PCA se parece. Os dados do PCA são plotados em verde e as linhas representam os dois vetores dos componentes principais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def projectPointsAndGetLines(data, components, xRange):\n",
    "    \"\"\"Project original data onto first component and get line details for top two components.\"\"\"\n",
    "    topComponent= components[:, 0]\n",
    "    slope1, slope2 = components[1, :2] / components[0, :2]\n",
    "\n",
    "    means = data.mean()[:2]\n",
    "    demeaned = data.map(lambda v: v - means)\n",
    "    projected = demeaned.map(lambda v: (v.dot(topComponent) /\n",
    "                                        topComponent.dot(topComponent)) * topComponent)\n",
    "    remeaned = projected.map(lambda v: v + means)\n",
    "    x1,x2 = zip(*remeaned.collect())\n",
    "\n",
    "    lineStartP1X1, lineStartP1X2 = means - np.asarray([xRange, xRange * slope1])\n",
    "    lineEndP1X1, lineEndP1X2 = means + np.asarray([xRange, xRange * slope1])\n",
    "    lineStartP2X1, lineStartP2X2 = means - np.asarray([xRange, xRange * slope2])\n",
    "    lineEndP2X1, lineEndP2X2 = means + np.asarray([xRange, xRange * slope2])\n",
    "\n",
    "    return ((x1, x2), ([lineStartP1X1, lineEndP1X1], [lineStartP1X2, lineEndP1X2]),\n",
    "            ([lineStartP2X1, lineEndP2X1], [lineStartP2X2, lineEndP2X2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) = \\\n",
    "    projectPointsAndGetLines(correlatedData, topComponentsCorrelated, 5)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize=(7, 7))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
    "plt.plot(line1X1, line1X2, linewidth=3.0, c='#8cbfd0', linestyle='--')\n",
    "plt.plot(line2X1, line2X2, linewidth=3.0, c='#d6ebf2', linestyle='--')\n",
    "plt.scatter(dataCorrelated[:,0], dataCorrelated[:,1], s=14**2, c='#d6ebf2',\n",
    "            edgecolors='#8cbfd0', alpha=0.75)\n",
    "plt.scatter(x1, x2, s=14**2, c='#62c162', alpha=.75)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) = \\\n",
    "    projectPointsAndGetLines(randomData, topComponentsRandom, 5)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize=(7, 7))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
    "plt.plot(line1X1, line1X2, linewidth=3.0, c='#8cbfd0', linestyle='--')\n",
    "plt.plot(line2X1, line2X2, linewidth=3.0, c='#d6ebf2', linestyle='--')\n",
    "plt.scatter(dataRandom[:,0], dataRandom[:,1], s=14**2, c='#d6ebf2',\n",
    "            edgecolors='#8cbfd0', alpha=0.75)\n",
    "plt.scatter(x1, x2, s=14**2, c='#62c162', alpha=.75)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2c) Explicação da Variância**\n",
    "\n",
    "#### Finalmente, vamos quantificar o quanto da variância foi capturado pelo PCA em cada um dos dados analisados. Para isso, vamos computar a razão entre a soma dos $k$ autovalores utilizados pela soma de todos os autovalores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def varianceExplained(data, k=1):\n",
    "    \"\"\"Calculate the fraction of variance explained by the top `k` eigenvectors.\n",
    "\n",
    "    Args:\n",
    "        data (RDD of np.ndarray): An RDD that contains NumPy arrays which store the\n",
    "            features for an observation.\n",
    "        k: The number of principal components to consider.\n",
    "\n",
    "    Returns:\n",
    "        float: A number between 0 and 1 representing the percentage of variance explained\n",
    "            by the top `k` eigenvectors.\n",
    "    \"\"\"\n",
    "    <COMPLETAR>\n",
    "    return eigenvalues[:k].sum()/eigenvalues.sum()\n",
    "\n",
    "varianceRandom1 = varianceExplained(randomData, 1)\n",
    "varianceCorrelated1 = varianceExplained(correlatedData, 1)\n",
    "varianceRandom2 = varianceExplained(randomData, 2)\n",
    "varianceCorrelated2 = varianceExplained(correlatedData, 2)\n",
    "print ('Percentage of variance explained by the first component of randomData: {0:.1f}%'\n",
    "       .format(varianceRandom1 * 100))\n",
    "print ('Percentage of variance explained by both components of randomData: {0:.1f}%'\n",
    "       .format(varianceRandom2 * 100))\n",
    "print ('\\nPercentage of variance explained by the first component of correlatedData: {0:.1f}%'.\n",
    "       format(varianceCorrelated1 * 100))\n",
    "print ('Percentage of variance explained by both components of correlatedData: {0:.1f}%'\n",
    "       .format(varianceCorrelated2 * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Variance explained (2d)\n",
    "Test.assertTrue(np.allclose(varianceRandom1, 0.588017172066), 'incorrect value for varianceRandom1')\n",
    "Test.assertTrue(np.allclose(varianceCorrelated1, 0.933608329586),\n",
    "                'incorrect value for varianceCorrelated1')\n",
    "Test.assertTrue(np.allclose(varianceRandom2, 1.0), 'incorrect value for varianceRandom2')\n",
    "Test.assertTrue(np.allclose(varianceCorrelated2, 1.0), 'incorrect value for varianceCorrelated2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### **Part 3:  Aplicação do PCA na base Million Song, com aplicação de Regressão Linear na base projetada **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3a) Carregando os dados do Data Set**\n",
    "\n",
    "#### Vamos repetir os procedimentos realizados no Lab4a para carregar e fazer o parsing da base de dados. Também vamos reescrever algumas das funções utilitárias daquele lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseDir = os.path.join('Data')\n",
    "inputPath = os.path.join('Aula04', 'millionsong.txt')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "numPartitions = 2\n",
    "rawData = sc.textFile(fileName, numPartitions)\n",
    "\n",
    "print rawData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parsePoint(line):\n",
    "    \"\"\"Converts a comma separated unicode string into a `LabeledPoint`.\n",
    "\n",
    "    Args:\n",
    "        line (unicode): Comma separated unicode string where the first element is the label and the\n",
    "            remaining elements are features.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: The line is converted into a `LabeledPoint`, which consists of a label and\n",
    "            features.\n",
    "    \"\"\"\n",
    "    Point = map(float,line.split(','))\n",
    "    return LabeledPoint(Point[0]-1922,Point[1:])\n",
    "\n",
    "baselineRL = 17.017\n",
    "baselineInteract = 15.690\n",
    "\n",
    "millionRDD = rawData.map(parsePoint)\n",
    "print millionRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squaredError(label, prediction):\n",
    "    \"\"\"Calculates the the squared error for a single prediction.\n",
    "\n",
    "    Args:\n",
    "        label (float): The correct value for this observation.\n",
    "        prediction (float): The predicted value for this observation.\n",
    "\n",
    "    Returns:\n",
    "        float: The difference between the `label` and `prediction` squared.\n",
    "    \"\"\"\n",
    "    return np.square(label-prediction)\n",
    "\n",
    "def calcRMSE(labelsAndPreds):\n",
    "    \"\"\"Calculates the root mean squared error for an `RDD` of (label, prediction) tuples.\n",
    "\n",
    "    Args:\n",
    "        labelsAndPred (RDD of (float, float)): An `RDD` consisting of (label, prediction) tuples.\n",
    "\n",
    "    Returns:\n",
    "        float: The square root of the mean of the squared errors.\n",
    "    \"\"\"\n",
    "    return np.sqrt(labelsAndPreds.map(lambda rec: squaredError(rec[0],rec[1])).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3b) PCA do LabeledPoint**\n",
    "\n",
    "#### Vamos reescrever a função `pca`, agora denominada `pcaLP` para receber um RDD de LabeledPoints. Será necessário realizar duas alterações: o RDD a ser passado para `estimateCovariance` deve ser uma transformação de `data` para conter apenas os `features` do LabeledPoint. O retorno da função deve transformar apenas o `features` utilizando os autovetores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pcaLP(data, k=2):\n",
    "    \"\"\"Computes the top `k` principal components, corresponding scores, and all eigenvalues.\n",
    "\n",
    "    Note:\n",
    "        All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n",
    "        each eigenvectors as a column.  This function should also return eigenvectors as columns.\n",
    "\n",
    "    Args:\n",
    "        data (RDD of np.ndarray): An `RDD` consisting of NumPy arrays.\n",
    "        k (int): The number of principal components to return.\n",
    "\n",
    "    Returns:\n",
    "        tuple of (np.ndarray, RDD of np.ndarray, np.ndarray): A tuple of (eigenvectors, `RDD` of\n",
    "            scores, eigenvalues).  Eigenvectors is a multi-dimensional array where the number of\n",
    "            rows equals the length of the arrays in the input `RDD` and the number of columns equals\n",
    "            `k`.  The `RDD` of scores has the same number of rows as `data` and consists of arrays\n",
    "            of length `k`.  Eigenvalues is an array of length d (the number of features).\n",
    "    \"\"\"\n",
    "    cov = estimateCovariance(data.map(lambda x: x.features))\n",
    "    eigVals, eigVecs = eigh(cov)\n",
    "    inds = np.argsort(-eigVals)\n",
    "    vecs = eigVecs[:,inds[:k]]\n",
    "    vals = eigVals[inds[:cov.shape[0]]]\n",
    "\n",
    "    # Return the `k` principal components, `k` scores, and all eigenvalues\n",
    "    return data.map(lambda x: LabeledPoint(x.label,x.features.dot(vecs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3c) Determinando o valor de k**\n",
    "\n",
    "#### Vamos utilizar a função `varianceExplained` aplicada em `millionRDD` (dica: deve primeiro realizar uma transformação) para determinar o valor de k que utilizaremos para os testes de Regressão Linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in range(1,10):\n",
    "    varexp = varianceExplained(millionRDD.map(lambda x: x.features), k)\n",
    "    print 'Variation explained by {} components is {}'.format(k,varexp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3d) Regressão Linear**\n",
    "\n",
    "#### Vamos aplicar a regressão linear do Lab 4a para o RDD do PCA do million song data para k = 2, k=6 e k=8. Os resultados serão comparados entre si e com os resultados obtidos no Lab 4a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "# Values to use when training the linear regression model\n",
    "numIters = 2000  # iterations\n",
    "alpha = 1.0  # step\n",
    "miniBatchFrac = 1.0  # miniBatchFraction\n",
    "reg = 1e-1  # regParam\n",
    "regType = None#'l2'  # regType\n",
    "useIntercept = True  # intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Run pca using scaledData\n",
    "pcaMillionRDD = pcaLP(millionRDD, 2)\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "parsedTrainData, parsedValData, parsedTestData = pcaMillionRDD.randomSplit(weights, seed)\n",
    "\n",
    "pcaModel = LinearRegressionWithSGD.train(parsedTrainData, iterations = numIters, step = alpha, miniBatchFraction = 1.0,\n",
    "                                          regParam=reg,regType=regType, intercept=useIntercept)\n",
    "labelsAndPreds = parsedValData.map(lambda lp: (lp.label, pcaModel.predict(lp.features)))\n",
    "rmseValLPCA2 = calcRMSE(labelsAndPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcaMillionRDD = pcaLP(millionRDD, 6)\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "parsedTrainData, parsedValData, parsedTestData = pcaMillionRDD.randomSplit(weights, seed)\n",
    "\n",
    "pcaModel = LinearRegressionWithSGD.train(parsedTrainData, iterations = numIters, step = alpha, miniBatchFraction = 1.0,\n",
    "                                          regParam=reg,regType=regType, intercept=useIntercept)\n",
    "labelsAndPreds = parsedValData.map(lambda lp: (lp.label, pcaModel.predict(lp.features)))\n",
    "rmseValLPCA6 = calcRMSE(labelsAndPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcaMillionRDD = pcaLP(millionRDD, 8)\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "parsedTrainData, parsedValData, parsedTestData = pcaMillionRDD.randomSplit(weights, seed)\n",
    "\n",
    "pcaModel = LinearRegressionWithSGD.train(parsedTrainData, iterations = numIters, step = alpha, miniBatchFraction = 1.0,\n",
    "                                          regParam=reg,regType=regType, intercept=useIntercept)\n",
    "labelsAndPreds = parsedValData.map(lambda lp: (lp.label, pcaModel.predict(lp.features)))\n",
    "rmseValLPCA8 = calcRMSE(labelsAndPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('Validation RMSE:\\n\\tLinear Regression (orig.) = {0:.3f}\\n\\tLinear Regression Interact = {1:.3f}' +\n",
    "       '\\n\\tLR PCA (k=2) = {2:.3f}\\n\\tLR PCA (k=6) = {3:.3f}\\n\\tLR PCA (k=8) = {4:.3f}' \n",
    "      ).format(baselineRL, baselineInteract, rmseValLPCA2, rmseValLPCA6, rmseValLPCA8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
