{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CMCC](http://cmcc.ufabc.edu.br/images/logo_site.jpg)\n",
    "\n",
    "# **Lab 5b - k-Means para Quantização de Atributos**\n",
    "\n",
    "#### Os algoritmos de agrupamento de dados, além de serem utilizados em análise exploratória para extrair padrões de similaridade entre os objetos, pode ser utilizado para compactar o espaço de dados.\n",
    "\n",
    "#### Neste notebook vamos utilizar nossa base de dados de Sentiment Movie Reviews para os experimentos. Primeiro iremos utilizar a técnica word2vec que aprende uma transformação dos tokens de uma base em um vetor de atributos. Em seguida, utilizaremos o algoritmo k-Means para compactar a informação desses atributos e projetar cada objeto em um espaço de atributos de tamanho fixo.\n",
    "\n",
    "#### As células-exercícios iniciam com o comentário `# EXERCICIO` e os códigos a serem completados estão marcados pelos comentários `<COMPLETAR>`.\n",
    "\n",
    "#### ** Nesse notebook: **\n",
    "#### *Parte 1:* Word2Vec\n",
    "#### *Parte 2:* k-Means para quantizar os atributos\n",
    "#### *Parte 3:* Aplicando um k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 0: Preliminares**\n",
    "\n",
    "#### Para este notebook utilizaremos a base de dados Movie Reviews que será utilizada para o segundo projeto.\n",
    "\n",
    "#### A base de dados tem os campos separados por '\\t' e o seguinte formato:\n",
    "   `\"id da frase\",\"id da sentença\",\"Frase\",\"Sentimento\"`\n",
    "\n",
    "#### Para esse laboratório utilizaremos apenas o campo \"Frase\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def parseRDD(point):\n",
    "    \"\"\" Parser for the current dataset. It receives a data point and return\n",
    "        a sentence (third field).\n",
    "    Args:\n",
    "        point (str): input data point\n",
    "    Returns:\n",
    "        str: a string\n",
    "    \"\"\"    \n",
    "    data = point.split('\\t')\n",
    "    return (int(data[0]),data[2])\n",
    "\n",
    "def notempty(point):\n",
    "    \"\"\" Returns whether the point string is not empty\n",
    "    Args:\n",
    "        point (str): input string\n",
    "    Returns:\n",
    "        bool: True if it is not empty\n",
    "    \"\"\"   \n",
    "    return len(point[1])>0\n",
    "\n",
    "filename = os.path.join(\"Data\",\"Aula04\",\"MovieReviews2.tsv\")\n",
    "rawRDD = sc.textFile(filename,100)\n",
    "header = rawRDD.take(1)[0]\n",
    "\n",
    "dataRDD = (rawRDD\n",
    "           #.sample(False, 0.1, seed=42)\n",
    "           .filter(lambda x: x!=header)\n",
    "           .map(parseRDD)\n",
    "           .filter(notempty)\n",
    "           #.sample( False, 0.1, 42 )\n",
    "           )\n",
    "\n",
    "print 'Read {} lines'.format(dataRDD.count())\n",
    "print 'Sample line: {}'.format(dataRDD.takeSample(False, 1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 1: Word2Vec**\n",
    "\n",
    "#### A técnica [word2vec][word2vec] aprende através de uma rede neural semântica uma representação vetorial de cada token em um corpus de tal forma que palavras semanticamente similares sejam similares na representação vetorial.\n",
    "\n",
    "####  O PySpark contém uma implementação dessa técnica, para aplicá-la basta passar um RDD em que cada objeto representa um documento e cada documento é representado por uma lista de tokens na ordem em que aparecem originalmente no corpus. Após o processo de treinamento, podemos transformar um token utilizando o método [`transform`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.feature) para transformar cada token em uma representaçã vetorial.\n",
    "\n",
    "#### Nesse ponto, cada objeto de nossa base será representada por uma matriz de tamanho variável.\n",
    "\n",
    "[word2vec]: https://code.google.com/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1a) Gerando RDD de tokens**\n",
    "\n",
    "#### Utilize a função de tokenização `tokenize` do Lab4d para gerar uma RDD `wordsRDD` contendo listas de tokens da nossa base original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "import re\n",
    "\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "stopfile = os.path.join(\"Data\",\"Aula04\",\"stopwords.txt\")\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    return <COMPLETAR>\n",
    "\n",
    "wordsRDD = dataRDD.map(lambda x: tokenize(x[1]))\n",
    "\n",
    "print wordsRDD.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Tokenize a String (1a)\n",
    "assert wordsRDD.take(1)[0]==[u'quiet', u'introspective', u'entertaining', u'independent', u'worth', u'seeking'], 'lista incorreta!'\n",
    "print 'ok!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1b) Aplicando transformação word2vec**\n",
    "\n",
    "#### Crie um modelo word2vec aplicando o método `fit` na RDD criada no exercício anterior.\n",
    "\n",
    "#### Para aplicar esse método deve ser fazer um pipeline de métodos, primeiro executando `Word2Vec()`, em seguida aplicando o método `setVectorSize()` com o tamanho que queremos para nosso vetor (utilize tamanho 5), seguido de `setSeed()` para a semente aleatória, em caso de experimentos controlados (utilizaremos 42) e, finalmente, `fit()` com nossa `wordsRDD` como parâmetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "model = Word2Vec().<COMPLETAR>\n",
    "\n",
    "print model.transform(u'entertaining')\n",
    "print model.findSynonyms(u'entertaining', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist = np.abs(model.transform(u'entertaining')-np.array([-0.246186971664,-0.127226486802,0.0271916668862,0.0112947737798,-0.206053063273])).mean()\n",
    "assert dist<1e-6, 'valores incorretos'\n",
    "print 'ok!'\n",
    "assert model.findSynonyms(u'entertaining', 1)[0][0] == 'affair', 'valores incorretos'\n",
    "print 'ok!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1c) Gerando uma RDD de matrizes**\n",
    "\n",
    "#### Como primeiro passo, precisamos gerar um dicionário em que a chave são as palavras e o valor é o vetor representativo dessa palavra.\n",
    "\n",
    "#### Para isso vamos primeiro gerar uma lista `uniqueWords` contendo as palavras únicas do RDD words, removendo aquelas que aparecem menos do que 5 vezes [$^1$](#1). Em seguida, criaremos um dicionário `w2v` que a chave é um token e o valor é um `np.array` do vetor transformado daquele token[$^2$](#2).\n",
    "\n",
    "#### Finalmente, vamos criar uma RDD chamada `vectorsRDD` em que cada registro é representado por uma matriz onde cada linha representa uma palavra transformada.\n",
    "\n",
    "##### 1\n",
    "Na versão 1.3 do PySpark o modelo Word2Vec utiliza apenas os tokens que aparecem mais do que 5 vezes no corpus, na versão 1.4 isso é parametrizado.\n",
    "\n",
    "##### 2\n",
    "Na versão 1.4 do PySpark isso pode ser feito utilizando o método `getVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "uniqueWords = (wordsRDD\n",
    "               .<COMPLETAR>\n",
    "               .<COMPLETAR>\n",
    "               .<COMPLETAR>\n",
    "               .<COMPLETAR>\n",
    "               .collect()\n",
    "               )\n",
    "\n",
    "print '{} tokens únicos'.format(len(uniqueWords))\n",
    "\n",
    "w2v = {}\n",
    "for w in uniqueWords:\n",
    "    w2v[w] = <COMPLETAR>\n",
    "w2vb = sc.broadcast(w2v)       \n",
    "print 'Vetor entertaining: {}'.format( w2v[u'entertaining'])\n",
    "\n",
    "vectorsRDD = (wordsRDD\n",
    "              .<COMPLETAR>\n",
    "             )\n",
    "recs = vectorsRDD.take(2)\n",
    "firstRec, secondRec = recs[0], recs[1]\n",
    "print firstRec.shape, secondRec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Tokenizing the small datasets (1c)\n",
    "assert len(uniqueWords) == 3332,  'valor incorreto'\n",
    "print 'ok!'\n",
    "\n",
    "assert np.mean(np.abs(w2v[u'entertaining']-[-0.24618697, -0.12722649,  0.02719167,  0.01129477, -0.20605306]))<1e-6,'valor incorreto'\n",
    "print 'ok!'\n",
    "\n",
    "assert secondRec.shape == (10,5)\n",
    "print 'ok!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 2: k-Means para quantizar os atributos**\n",
    "\n",
    "#### Nesse momento é fácil perceber que não podemos aplicar nossas técnicas de aprendizado supervisionado nessa base de dados:\n",
    "\n",
    "  * #### A regressão logística requer um vetor de tamanho fixo representando cada objeto\n",
    "  * #### O k-NN necessita uma forma clara de comparação entre dois objetos, que métrica de similaridade devemos aplicar?\n",
    "  \n",
    "#### Para resolver essa situação, vamos executar uma nova transformação em nossa RDD.   Primeiro vamos aproveitar o fato de que dois tokens com significado similar são mapeados em vetores similares, para agrupá-los em um atributo único.\n",
    "\n",
    "#### Ao aplicarmos o k-Means nesse conjunto de vetores, podemos criar $k$ pontos representativos e, para cada documento, gerar um histograma de contagem de tokens nos clusters gerados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2a) Agrupando os vetores e criando centros representativos**\n",
    "\n",
    "#### Como primeiro passo vamos gerar um RDD com os valores do dicionário `w2v`. Em seguida, aplicaremos o algoritmo `k-Means` com $k = 200$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "from  pyspark.mllib.clustering import KMeans\n",
    "\n",
    "vectors2RDD = sc.parallelize(np.array(w2v.values()),1)\n",
    "print 'Sample vector: {}'.format(vectors2RDD.take(1))\n",
    "\n",
    "modelK = KMeans.<COMPLETAR>\n",
    "\n",
    "clustersRDD = vectors2RDD.<COMPLETAR>\n",
    "print '10 first clusters allocation: {}'.format(clustersRDD.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Amazon record with the most tokens (1d)\n",
    "assert clustersRDD.take(10)==[134, 126, 209, 221, 401, 485, 197, 269, 296, 265], 'valor incorreto'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2b) Transformando matriz de dados em vetores quantizados**\n",
    "\n",
    "#### O próximo passo consiste em transformar nosso RDD de frases em um RDD de pares (id, vetor quantizado). Para isso vamos criar uma função quantizador que receberá como parâmetros o objeto, o modelo de k-means, o valor de k e o dicionário word2vec.\n",
    "\n",
    "#### Para cada ponto, vamos separar o id e aplicar a função `tokenize` na string. Em seguida, transformamos a lista de tokens em uma matriz word2vec. Finalmente, aplicamos cada vetor dessa matriz no modelo de k-Means, gerando um vetor de tamanho $k$ em que cada posição $i$ indica quantos tokens pertencem ao cluster $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def quantizador(point, model, k, w2v):\n",
    "    key = <COMPLETAR>\n",
    "    words = <COMPLETAR>\n",
    "    matrix = np.array( <COMPLETAR> )\n",
    "    features = np.zeros(k)\n",
    "    for v in matrix:\n",
    "        c = <COMPLETAR>\n",
    "        features[c] += 1\n",
    "    return (key, features)\n",
    "    \n",
    "quantRDD = dataRDD.map(lambda x: quantizador(x, modelK, 500, w2v))\n",
    "\n",
    "print quantRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Implement a TF function (2a)\n",
    "assert quantRDD.take(1)[0][1].sum() == 5, 'valores incorretos'\n",
    "print 'ok!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3: Aplicando k-NN**\n",
    "\n",
    "#### Nessa parte, vamos testar o algoritmo k-NN para verificar se a base quantizada é capaz de capturar a semelhança entre documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4a) Pré-calcule a norma dos vetores**\n",
    "\n",
    "#### Da mesma forma que no Lab 4c, calcule a norma dos vetores de `quantRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataNorms = quantRDD.map(lambda rec: (rec[0],np.sqrt(rec[1].dot(rec[1]))))\n",
    "dataNormsBroadcast = sc.broadcast(dataNorms.collectAsMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4b) Calcule a similaridade do cosseno entre pares de registros**\n",
    "\n",
    "#### Complete a função `cosineSim` que receberá um par ( (doc1, vec1), (doc2, vec2) ) para calcular a similaridade do cosseno entre esses dois documentos.\n",
    "\n",
    "* #### Usando o índice invertido, agrupe a base pela chave (token) e aplique a função `genList` que deve gerar uma lista de tuplas (token, ((doc1,tfidf),(doc2,tfidf)) de todos os pares de documentos com essa token em comum exceto nos casos `doc1==doc2`.\n",
    "* #### Em seguida, gere tuplas do tipo ( (doc1, doc2), tfidf1*tfidf2/(norma1*norma2) ) a reduza para realizar a somatória desses valores sob a mesma chave.\n",
    "\n",
    "#### Dessa forma teremos os registros de pares de documentos que possuem similaridade não nula com sua similaridade calculada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "from itertools import product\n",
    "\n",
    "def calcsim(rec):\n",
    "    items = list(rec[1])\n",
    "    return <COMPLETAR>\n",
    "    \n",
    "newRDD = (quantRDD\n",
    "          .<COMPLETAR>\n",
    "          .<COMPLETAR>\n",
    "          .<COMPLETAR>\n",
    "          .<COMPLETAR>\n",
    "          .<COMPLETAR>\n",
    "          .cache()\n",
    "          )\n",
    "\n",
    "newcount = newRDD.count()\n",
    "print newcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert newcount==11796442, 'incorrect value'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4f) k-NN**\n",
    "\n",
    "#### Vamos gerar agora a lista dos *k* documentos mais similares de cada documento.\n",
    "\n",
    "* #### Gere uma RDD partindo da `commonTokens` de tal forma a ter ( id1, (id2, sim) )\n",
    "* #### Agrupe pela chave e transforme a RDD para ( id1, [ (id,sim) ] ) onde a lista deve ter k elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def genklist(rec,k):\n",
    "    \"\"\" Generate the list of the k most similar documents to the key\n",
    "    Args:\n",
    "        record: a pair, (doc, [(doc,sim)])\n",
    "        k: number of most similar elements\n",
    "    Returns:\n",
    "        pair: (doc, [(doc,sim)])\n",
    "    \"\"\"\n",
    "    <COMPLETAR>\n",
    "    return (key, docs[:k])\n",
    "    \n",
    "def knn(simRDD, k):\n",
    "    \"\"\" Generate the knn RDD for a given RDD.\n",
    "    Args:\n",
    "        simRDD: RDD of ( (doc1,doc2), sim)\n",
    "        k: number of most similar elements\n",
    "    Returns:\n",
    "        RDD: RDD of ( doc1, [docs, sims])\n",
    "    \"\"\"\n",
    "\n",
    "    ksimRDD = (simRDD\n",
    "               .<COMPLETAR>\n",
    "               .<COMPLETAR>\n",
    "               .<COMPLETAR>\n",
    "              )\n",
    "    return ksimRDD\n",
    "\n",
    "ksimReviewsRDD = knn(newRDD, 3)\n",
    "ksimReviewsRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dataRDD.filter(lambda x: x[0] in [55300,39009,130973,66284]).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
